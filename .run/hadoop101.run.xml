<component name="ProjectRunConfigurationManager">
  <configuration default="false" name="hadoop101" type="SparkSubmitConfigurationType" factoryName="SshSparkJobConfigurationType" show_console_on_std_err="false" show_console_on_std_out="false">
    <option name="allowRunningInParallel" value="false" />
    <option name="archives">
      <list />
    </option>
    <option name="artifactArgs" value="" />
    <option name="artifactPath">
      <FilePath>
        <option name="path" value="$PROJECT_DIR$/spark/target/spark-jar-with-dependencies.jar" />
        <option name="type" value="UPLOAD" />
      </FilePath>
    </option>
    <option name="beforeShellScript" value="" />
    <option name="className" value="com.dahuatech.spark.demo.SparkSubmitRemoteDemo" />
    <option name="clusterManager" value="YARN" />
    <option name="conf" value="spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j.properties spark.executor.extraJavaOptions=-Dlog4j.configuration=/log4j.properties " />
    <option name="deployMode" value="CLUSTER" />
    <option name="driverClassPath">
      <list />
    </option>
    <option name="driverCores" value="1" />
    <option name="driverJavaOptions" value="" />
    <option name="driverLibraryPath">
      <list />
    </option>
    <option name="driverMemory" value="1G" />
    <option name="envParams" value="" />
    <option name="excludePackages" value="" />
    <option name="executorCores" value="3" />
    <option name="executorMemory" value="3G" />
    <option name="files">
      <list>
        <FilePath>
          <option name="path" value="/root/spark-remote-home/log4j.properties" />
          <option name="type" value="SERVER" />
        </FilePath>
      </list>
    </option>
    <option name="interactive" value="false" />
    <option name="jars">
      <list />
    </option>
    <option name="keytab">
      <FilePath>
        <option name="path" value="" />
        <option name="type" value="CUSTOM" />
      </FilePath>
    </option>
    <option name="master" value="local" />
    <option name="numExecutors" value="3" />
    <option name="packages" value="" />
    <option name="principal" value="" />
    <option name="projectPathOnTarget" />
    <option name="propertiesFile">
      <FilePath>
        <option name="path" value="" />
        <option name="type" value="CUSTOM" />
      </FilePath>
    </option>
    <option name="proxyUser" value="" />
    <option name="pyFiles">
      <list />
    </option>
    <option name="queue" value="" />
    <option name="repositories" value="" />
    <option name="selectedOptions">
      <list />
    </option>
    <option name="shellExecutor" value="/bin/bash" />
    <option name="sparkHome" value="/opt/mod/spark-3.0.0-bin-hadoop3.2" />
    <option name="sparkMonitoringDriverId" value="" />
    <option name="sshConfigId" value="3b5ba337-756f-404e-af11-4d2f4898b3f6" />
    <option name="supervise" value="false" />
    <option name="targetDirectory">
      <FilePath>
        <option name="path" value="/root/spark-remote-home" />
        <option name="type" value="SERVER" />
      </FilePath>
    </option>
    <option name="totalExecutorCores" value="" />
    <option name="verbose" value="false" />
    <option name="workDirectoryPath">
      <FilePath>
        <option name="path" value="" />
        <option name="type" value="SERVER" />
      </FilePath>
    </option>
    <method v="2">
      <option name="Maven.BeforeRunTask" enabled="true" file="$PROJECT_DIR$/spark/pom.xml" goal="clean package" />
      <option name="SftpSparkFileUpload" enabled="true" />
    </method>
  </configuration>
</component>